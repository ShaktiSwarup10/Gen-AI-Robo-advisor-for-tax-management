{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"yJ4cs-h3GMao"}},{"cell_type":"code","source":["!pip install openai\n","!pip install -qU pypdf\n","!pip install langchain langchain-community langchain-openai\n","!pip install sentence-transformers faiss-cpu rank_bm25"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"4HXbqNfKVn9P","executionInfo":{"status":"ok","timestamp":1734361210712,"user_tz":0,"elapsed":37498,"user":{"displayName":"Gladys Mawarni","userId":"03879441175200282144"}},"outputId":"a0fa543d-4aef-4e7b-894d-6d453aa7a09f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.5)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.11)\n","Collecting langchain-community\n","  Downloading langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n","Collecting langchain-openai\n","  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.24)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n","Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Collecting langchain\n","  Downloading langchain-0.3.12-py3-none-any.whl.metadata (7.1 kB)\n","Collecting langchain-core<0.4.0,>=0.3.24 (from langchain)\n","  Downloading langchain_core-0.3.25-py3-none-any.whl.metadata (6.3 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n","  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n","Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain)\n","  Downloading langchain_text_splitters-0.3.3-py3-none-any.whl.metadata (2.3 kB)\n","Collecting openai<2.0.0,>=1.55.3 (from langchain-openai)\n","  Downloading openai-1.57.4-py3-none-any.whl.metadata (24 kB)\n","Collecting tiktoken<1,>=0.7 (from langchain-openai)\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (0.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (4.66.6)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai) (1.2.2)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain) (3.0.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Downloading langchain_community-0.3.12-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain-0.3.12-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_openai-0.2.12-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading langchain_core-0.3.25-py3-none-any.whl (411 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_text_splitters-0.3.3-py3-none-any.whl (27 kB)\n","Downloading openai-1.57.4-py3-none-any.whl (390 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.3/390.3 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n","Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, tiktoken, pydantic-settings, openai, dataclasses-json, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n","  Attempting uninstall: openai\n","    Found existing installation: openai 1.54.5\n","    Uninstalling openai-1.54.5:\n","      Successfully uninstalled openai-1.54.5\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.24\n","    Uninstalling langchain-core-0.3.24:\n","      Successfully uninstalled langchain-core-0.3.24\n","  Attempting uninstall: langchain-text-splitters\n","    Found existing installation: langchain-text-splitters 0.3.2\n","    Uninstalling langchain-text-splitters-0.3.2:\n","      Successfully uninstalled langchain-text-splitters-0.3.2\n","  Attempting uninstall: langchain\n","    Found existing installation: langchain 0.3.11\n","    Uninstalling langchain-0.3.11:\n","      Successfully uninstalled langchain-0.3.11\n","Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.12 langchain-community-0.3.12 langchain-core-0.3.25 langchain-openai-0.2.12 langchain-text-splitters-0.3.3 marshmallow-3.23.1 mypy-extensions-1.0.0 openai-1.57.4 pydantic-settings-2.7.0 python-dotenv-1.0.1 tiktoken-0.8.0 typing-inspect-0.9.0\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n","Collecting rank_bm25\n","  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.5)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n","Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n","Installing collected packages: rank_bm25, faiss-cpu\n","Successfully installed faiss-cpu-1.9.0.post1 rank_bm25-0.2.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhWQLzIWKBwc"},"outputs":[],"source":["# Libraries\n","import yfinance as yf\n","import pandas as pd\n","import os\n","import json\n","import numpy as np\n","import openai\n","\n","from IPython.display import display, Markdown"]},{"cell_type":"code","source":["# Import the userdata module from Google Colab\n","from google.colab import userdata\n","# Retrieve the API key\n","api_key = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"6yzu6nxPVjgM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set the key in environment\n","os.environ['OPENAI_API_KEY'] = api_key\n","client = openai.OpenAI()"],"metadata":{"id":"BQjwY2ifVlN6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. User portfolio data"],"metadata":{"id":"Dc2A-MgXAG33"}},{"cell_type":"code","source":["def save_user_portfolio(user_name, asset_name, units, avg_cost):\n","    # Convert user_name to lowercase for consistency\n","    user_name = user_name.lower()\n","    # Replace spaces in user_name with underscores for file naming\n","    user_name = '_'.join(user_name.split())\n","\n","    # Create the 'data' directory if it does not exist\n","    data_folder = 'data'\n","    if not os.path.exists(data_folder):\n","        os.makedirs(data_folder)  # Create the folder if it doesn't exist\n","\n","    # Define the file path using the user_name to make it identifiable\n","    file_path = os.path.join(data_folder, f\"{user_name}_portfolio.json\")\n","\n","    # Create a dictionary to store the asset data\n","    portfolio_data = {\n","        \"asset_name\": asset_name.upper(),  # Store asset name in uppercase for consistency\n","        \"units\": units,  # Store the number of units for the asset\n","        \"avg_cost\": avg_cost  # Store the average cost of the asset\n","    }\n","\n","    # If the file already exists, load existing data and update it\n","    if os.path.exists(file_path):\n","        with open(file_path, 'r') as file:\n","            try:\n","                # Try to load existing data from the JSON file\n","                existing_data = json.load(file)\n","            except json.JSONDecodeError:\n","                # If the JSON is invalid, initialize with an empty dictionary\n","                existing_data = {}\n","    else:\n","        # If the file doesn't exist, start with an empty dictionary\n","        existing_data = {}\n","\n","    # Update the portfolio with the new asset data, using the asset name as the key\n","    existing_data[asset_name.upper()] = portfolio_data\n","\n","    # Write the updated portfolio data back to the file in JSON format\n","    with open(file_path, 'w') as file:\n","        json.dump(existing_data, file, indent=4)  # Pretty-print with an indent of 4 spaces\n"],"metadata":{"id":"FDfHoKA_KU_S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Portfolio analysis"],"metadata":{"id":"010jY-BsATrk"}},{"cell_type":"code","source":["def get_stock_metrics(portfolio_data):\n","    # Extract the list of asset symbols from the portfolio data\n","    assets = list(portfolio_data.keys())\n","\n","    # Download adjusted closing prices for the assets over the past year\n","    price_data = yf.download(assets, period='1y')['Adj Close']\n","\n","    # Initialize an empty dictionary to store metrics for each stock\n","    metrics = {}\n","\n","    # Iterate through each asset to gather and store its metrics\n","    for i in range(len(assets)):\n","        # Get detailed information about the stock using the yfinance Ticker object\n","        stock_info = yf.Ticker(assets[i]).info\n","\n","        # Store various stock metrics for the asset in the metrics dictionary\n","        metrics[assets[i]] = {\n","            \"units_held\": portfolio_data[assets[i]]['units'],  # Number of units held\n","            \"avg_cost\": portfolio_data[assets[i]]['avg_cost'],  # Average cost of the asset\n","            \"current_price\": stock_info.get('currentPrice', 'N/A'),  # Current market price\n","            \"previous_close\": stock_info.get('previousClose', 'N/A'),  # Previous closing price\n","            \"52_week_high\": stock_info.get('fiftyTwoWeekHigh', 'N/A'),  # 52-week high price\n","            \"52_week_low\": stock_info.get('fiftyTwoWeekLow', 'N/A'),  # 52-week low price\n","            \"dividend_yield\": stock_info.get('dividendYield', 'N/A'),  # Dividend yield percentage\n","            \"market_cap\": stock_info.get('marketCap', 'N/A'),  # Market capitalization of the company\n","            \"pe_ratio\": stock_info.get('trailingPE', 'N/A'),  # Price-to-Earnings ratio\n","            \"eps\": stock_info.get('trailingEps', 'N/A'),  # Earnings per share\n","        }\n","\n","    # Return the historical price data and the calculated metrics for each stock\n","    return price_data, metrics\n"],"metadata":{"id":"z2vxutgDNws3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to calculate expected returns and covariance\n","def calculate_returns(prices):\n","  try:\n","    # Calculate daily returns\n","    returns = prices.pct_change().dropna()  # Returns should be a DataFrame\n","    # Calculate expected returns (mean) for each asset\n","    expected_returns = returns.mean()\n","    # Calculate the covariance matrix for the returns\n","    covariance_matrix = returns.cov()\n","    return expected_returns, covariance_matrix\n","\n","  except:\n","    # Only has 1 asset\n","    return None, None"],"metadata":{"id":"NwOZj5b68RIW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def portfolio_analysis(metrics, expected_returns, covariance_matrix):\n","    # Define a system prompt to instruct the AI model on its role and approach\n","    system_prompt = \"\"\"\n","    You are a portfolio manager responsible for analyzing and optimizing investment portfolios.\n","    Apply Modern Portfolio Theory where relevant, and use Earnings per Share (EPS) to achieve a balanced risk-return profile.\n","    Provide actionable insights to help users make informed investment decisions, focusing on practical recommendations rather than explaining portfolio metrics.\n","    \"\"\"\n","\n","    # Send a request to the OpenAI model (e.g., GPT-4) to generate an analysis\n","    response = client.chat.completions.create(\n","        model=\"gpt-4\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": system_prompt},  # Define the system's behavior and goals\n","            {\"role\": \"user\", \"content\": f\"Portfolio: {metrics}.\\nExpected returns:{expected_returns}, covariance matrix:{covariance_matrix}\"}\n","        ]\n","    )\n","\n","    # Return the content of the AI's response, which contains the portfolio analysis and insights\n","    return response.choices[0].message.content\n"],"metadata":{"id":"jpcde7KyGCWU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def portfolio_summary(user_name):\n","  user_name = user_name.lower()  # Convert to lowercase for consistency\n","  user_name = '_'.join(user_name.split())  # Replace spaces with underscores\n","\n","  # Define the file path for the user's portfolio\n","  file_path = os.path.join('data', f\"{user_name}_portfolio.json\")\n","\n","  # Check if the user's portfolio file exists\n","  if not os.path.exists(file_path):\n","      print(f\"No portfolio found for user: {user_name}\")\n","      return None, None, None\n","\n","  # Load the user's portfolio data\n","  with open(file_path, 'r') as file:\n","      portfolio_data = json.load(file)\n","\n","  # Initialize metrics\n","  total_value = 0\n","  total_gain_loss = 0\n","  summary = []\n","\n","  price_data, metrics = get_stock_metrics(portfolio_data)\n","\n","  for symbol in metrics:\n","    units = metrics[symbol][\"units_held\"]\n","    avg_cost = metrics[symbol]['avg_cost']\n","    current_price = metrics[symbol][\"current_price\"]\n","    # Calculate current value and gain/loss\n","    current_value = units * current_price\n","    gain_loss = (current_price - avg_cost) * units\n","\n","    # Update the total portfolio value and gain/loss\n","    total_value += current_value\n","    total_gain_loss += gain_loss\n","\n","    summary.append({\n","              \"asset_name\": symbol,\n","              \"units\": units,\n","              \"avg_cost\": avg_cost,\n","              \"current_price\": current_price,\n","              \"current_value\": current_value,\n","              \"gain_loss\": gain_loss\n","          })\n","\n","  # Calculate overall portfolio value and gain/loss\n","  overall_summary = {\n","      \"total_value\": total_value,\n","      \"total_gain_loss\": total_gain_loss,\n","      \"assets\": summary\n","  }\n","\n","  return overall_summary, price_data, metrics\n"],"metadata":{"id":"GbgWgKF3WB3f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Tax advisor"],"metadata":{"id":"bNpxoBo_AcZ7"}},{"cell_type":"code","source":["# Libraries\n","from langchain.retrievers import BM25Retriever, EnsembleRetriever\n","\n","from langchain_openai import ChatOpenAI\n","from langchain.chains import RetrievalQA\n","from langchain_community.document_loaders import PyPDFLoader\n","\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores.faiss import FAISS"],"metadata":{"id":"frq3WE4gBUnX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the tax law pdf\n","loader = PyPDFLoader(\"Germany-tax law.pdf\")\n","pages = loader.load_and_split()"],"metadata":{"id":"Xx6kMTIxFA5J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the BM25 retriever\n","bm25_retriever = BM25Retriever.from_documents(pages)\n","bm25_retriever.k =  2  # Retrieve top 2 results\n","# Use Hugging Face's Sentence Transformers model as the embedding model\n","embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","# embed the chunks as vectors and load them into the FAISS database\n","faiss_db = FAISS.from_documents(pages, embedding_model)\n","faiss_retriever = faiss_db.as_retriever(search_kwargs={\"k\": 2})\n","\n","# Initialize the ensemble retriever\n","ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever],\n","                                      weights=[0.4, 0.6])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":597,"referenced_widgets":["dbea7731030042268d5c56dd7b18dfbf","d1ff0903694a4878ac565aae763da944","eca8da91071b4ff3975532611f9d95fd","6dae556df6284058a6ee35dbd224674d","6533394147644f8d9856bdae01886d90","8bb5bcca14c345568b4ce8a0adda5a6c","0a60eb5676454697a3d22e254f3494d3","008bf1df76254215bc511455bd81c8b6","146d4aef59f04be98389e0a4a648be53","c15d53e463db410b84f34118f29e7074","b21e920757f14f379bf973a26d5119a6","f7bfee057c624661bc3cc7781572f7d9","c52e401b43a04c218339ebd103186445","836bafabc4ae438ab57a497826e0b3b8","1ecce74e0897409d95bc38f64a4469ab","440f5d510ba84835b851adb4df8202ee","2702f1044caf41bb9392ebb4300f8f3e","478ffadf269c4ea89adc1cc7fb4a235d","cc1cb21f1e454d6e8bca808340272066","b6836e5dcbbc4ff7ae023b9aaa5458fa","5fd045f313e4447299a626a908507d83","a5c8ad30b09240519cbe8d89e3cb0076","118175a6ab804615ac82a714ecc0dfe2","e0c24b916ee24125b36ead54a6443a23","7cd86b1174fb4d34a5def46751befe54","5a1e1b0244b7419097c4803d09589616","9e76e6b1ac3841438bef538f9a3c83f7","8ff0d80db27649f3bdc46a4dfd1f70fc","fa0350bb28584f81bd6b976bfc57db4f","bcdbe076846c446a815d091d7df58a85","d9ffa1dd024947da9d059d7b6c91d2bb","0d973be6b57f4c79a6ca65a6ff7a7e32","4e32dd7620934ea3a01b68845d687533","1dcf2cbf2a2e4b82837afcf8e038203f","5e731bf9afdf476c8142e69befda8b64","da91ad6714bb4fbeae1bacfffd0b2fd8","945e2158ef5848df813c51f462bb25f3","0a6463e32dde4335b225614b2bce86e6","23c5923166c9454f8ce01ac33d1f825f","582971837b044f96baf5949ceebbcef4","71cada4b63374429bdf1807e8dba9567","5645148d2f574aff8952fa1954143b67","a64836a328334ddc92dde41fb202196d","1aa277384db14c2a8519684024ddc218","c97a621e59ba4d8aaec24b359089a83a","d4e6bc5f3dbb4b69835a5eb5ad815d4b","7d5c27f44ffd48eb9e253859ecdac673","94019ecb7dd846e68c591c79aba1bac7","71848531f8544799a30a7188fe14e92b","c0923fa28264437e9bedc289eeff6166","d97486d0a46346b7adbcbdedbc7e27c4","36370a12dd5e49c49c03cb9d7a12d51e","d5ee8ad29bf14acbb00c58a51a6e2b13","34cc0e3b3cc249c1a4f0db46cdac7ec6","a35d132d64a44a7eb52cb80b5658f14b","5ee906fe3af644049563d0bd9dd1c326","650974bd953544c2b120d0d8e36bc3d1","5d7c798ced5849ac8dac6a008529ebe1","6cb7f060ce094575a4a2887138db4731","4ebfea72fd3040b893012944d75ac650","c620543bb85543268b4d1d7384a0b7aa","c5afdd6f4908487599338f1d64db3932","e528bb3edb594840ba7f96d58964bfed","92f38c2ffc1e4f7fa74f67163c253f94","3a520ba2464940bb9144988e945c3795","a5503afc62be42ea8c12462ed737f532","50572ff858e748179de1fe0587d6e29a","7daa5a965d324e5fb457ab80932dbe7f","a35a8486da734cd1a13f877c8924e863","c05852f80a844c349412c162a6c66de3","3340cf044d314fdc89e6e0dba5015bd1","cf25392bc3de4bec8da5cfb10a22cdb4","2a2f016904c94629aede7a611f26132e","77b8b29498fe4d9795d5a368a02a00de","65535dbb88ef480bb0c9118cc93df4e9","4c260807e0fb49ee96e39bd98dd05d0b","bf7815da00a1481db12c2175ea35813b","8b0ab386dfb048bca407a269b2a5c7bb","dd839778b7eb4f81a70c6b8027d008af","a64385cd68a843349e882447d9e4897a","8549d686bca34f30bb1b15a0b61ce657","1c709fe3bb4a4160ba2b261e19850055","9a0bf647d69a421182c7520932046559","dedee826c857402fa2e4f8f241ea3197","b555034b3d9649ee85f48710656f77cb","11d6409286624a7db98acaa20a884dc9","d4ef3c4fb4c24e2f9e0859cfdd760d01","3263a7498ba548028c5382dbf4347b65","6244d86c60d842488c25168b241480ef","4eaf30ebecc8439d899b3c545f78c2b7","efd4b096a26d4f25ade4cddf783bae30","cef4de44a4dc45c9ac3b9528ed5326c8","2ff3e81933b248fc8ad3c48b19d5c3e1","7ed43cd5e5d648438d3f04c59d52e9dc","be579d2eeeae473bb188736c8569218a","2c3b6c1d786741a9a23b6505ac918b7e","ff44e910274c46e4980966670860e83b","fbade2d4863d4d1992256a442fdef8e9","612f1ff131d14b58a64694944c1b02fe","7eacec959cef43eabee6103d6e81b1db","70102cee67f949a7b7b7f3fd0130b6df","0da38f0f48484ce38b158151ce7e8a57","78b1866c2c33459e8d17d277782bf9f6","41c6cd791cb84366b74bb61e6997a00b","21880c670a5d41c687095ffe30187ab1","a40434e681d8457c88ba14d7c73b9f8f","556fcbdbbf264ee2bbb94702c7899666","dffa316fbe2a4448bdb93d1eef38ca99","dbe2117f54c94c9c952ee2131796decf","67fac2efe37b43eda986b55b0f05436f","e376bdd4e1a649e1b50ac05c4868d751","07169ea1b6854010b1bb8d84a916ab3f","4d402144fbfe477c9103531abf833c8f","61f7708991944c57b60a48c7e78f2c1b","3b66a7db68d3429e805117c3d6ca4125","7210c14af9364fe99dfd1deaa60c18b1","9299da83085f47969375d939fd145fbd","77d61a9e38b54dc18a27ef9d710ff8aa","ae302a90a15f499f994d324828bfe809","cc394110f399406285b41393cf2e81ee","36874f392e3f4a2190fa9a8c1e047f2d"]},"id":"EypTbfB3K5_v","executionInfo":{"status":"ok","timestamp":1729073426196,"user_tz":-60,"elapsed":30696,"user":{"displayName":"Gladys Mawarni","userId":"03879441175200282144"}},"outputId":"9c0eaeff-14a8-4543-9065-70bd446ce6a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-13-cb21fb1fc6fb>:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n","  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm, trange\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbea7731030042268d5c56dd7b18dfbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7bfee057c624661bc3cc7781572f7d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"118175a6ab804615ac82a714ecc0dfe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dcf2cbf2a2e4b82837afcf8e038203f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c97a621e59ba4d8aaec24b359089a83a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ee906fe3af644049563d0bd9dd1c326"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50572ff858e748179de1fe0587d6e29a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b0ab386dfb048bca407a269b2a5c7bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6244d86c60d842488c25168b241480ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eacec959cef43eabee6103d6e81b1db"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e376bdd4e1a649e1b50ac05c4868d751"}},"metadata":{}}]},{"cell_type":"code","source":["def advisor(query, retriever):\n","    # Initialize a language model for conversation\n","    llm = ChatOpenAI()\n","\n","    # Set up a question-answering chain using the provided retriever for fetching relevant information\n","    # 'RetrievalQA' uses the LLM to answer queries based on the retrieved data\n","    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n","\n","    # Use the QA chain to process the query and generate a response\n","    response = qa.invoke(query)\n","\n","    # Return the result of the response, containing the answer to the query\n","    return response['result']\n"],"metadata":{"id":"TeRPOTScLM4D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## App"],"metadata":{"id":"2_-hOxgtAj1C"}},{"cell_type":"code","source":["from sys import breakpointhook\n","def start():\n","  # Get the user's nickname\n","  user_name = input(\"Enter your nickname: \")\n","\n","  # Prompt for user preference until a valid input is provided\n","  while True:\n","      pref = input(\"[1] update portfolio [2] evaluate portfolio [3] Consult tax law\\n\")\n","      if pref in [\"1\", \"2\", \"3\"]:\n","          break\n","      print(\"Invalid input. Please enter either 1, 2, or 3\")\n","\n","  # If the user chooses to update the portfolio\n","  if pref == \"1\":\n","      while True:\n","          asset_name = input(\"Enter the asset name: \")\n","          stock = yf.Ticker(asset_name)\n","          try:\n","              if len(stock.info) > 1:\n","                break\n","              else:\n","                print(\"Invalid input. Please enter a valid asset name.\")\n","          except Exception as e:\n","              print(f\"Error fetching data for {asset_name}: {e}. Please enter a valid asset name.\")\n","\n","      # Get the number of units\n","      while True:\n","          try:\n","              units = int(input(\"Enter the number of units: \"))\n","              if units < 0:\n","                  print(\"Number of units cannot be negative. Please try again.\")\n","                  continue\n","              break\n","          except ValueError:\n","              print(\"Invalid input. Please enter an integer value for the number of units.\")\n","\n","      # Get the average cost\n","      while True:\n","          try:\n","              avg_cost = float(input(\"Enter the average cost per unit (USD): \"))\n","              if avg_cost < 0:\n","                  print(\"Average cost cannot be negative. Please try again.\")\n","                  continue\n","              break\n","          except ValueError:\n","              print(\"Invalid input. Please enter a numeric value for the average cost.\")\n","\n","      # Save the updated portfolio data for the user\n","      save_user_portfolio(user_name, asset_name, units, avg_cost)\n","\n","  # If the user chooses to analyze their portfolio\n","  elif pref == \"2\":\n","      # Get a summary of the portfolio, along with price data and detailed metrics\n","      summary, price_data, metrics = portfolio_summary(user_name)\n","\n","      # If a summary exists, display the portfolio details\n","      if summary:\n","          print(f\"\\nPortfolio summary for {user_name}:\")\n","          print(f\"Total Portfolio Value: ${summary['total_value']:.2f}\")  # Show total portfolio value\n","          print(f\"Total Gain/Loss: ${summary['total_gain_loss']:.2f}\")  # Show overall gain/loss\n","\n","          # Loop through each asset in the portfolio and display its details\n","          for asset in summary['assets']:\n","              print(f\"Asset: {asset['asset_name']}, Units: {asset['units']}, \"\n","                    f\"Avg Cost: {asset['avg_cost']}, Current Price: {asset['current_price']}, \"\n","                    f\"Current Value: {asset['current_value']}, Gain/Loss: {round(asset['gain_loss'],2)}\")\n","\n","          # Calculate expected returns and the covariance matrix based on historical price data\n","          expected_returns, covariance_matrix = calculate_returns(price_data)\n","\n","          # Perform an analysis of the portfolio using the calculated metrics\n","          analysis = portfolio_analysis(metrics, expected_returns, covariance_matrix)\n","\n","          # Escape dollar signs for proper Markdown rendering\n","          analysis = analysis.replace('$', '\\$')\n","\n","          # Display the portfolio analysis as formatted Markdown text\n","          print(\"\\nPortfolio Analysis:\")\n","          display(Markdown(analysis))\n","\n","  # If the user chooses to consult tax law\n","  elif pref == \"3\":\n","      # Prompt the user to specify their legal concern\n","      query = input(\"Specify your legal concern:\\n\")\n","\n","      # Get a response from the advisor using the query and the ensemble retriever\n","      response = advisor(query, ensemble_retriever)\n","\n","      # Display the advisor's response as formatted Markdown text\n","      display(Markdown(response))\n","\n"],"metadata":{"id":"UMkncFjqXJ81"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nfzKzNMXVqam","executionInfo":{"status":"ok","timestamp":1729073470137,"user_tz":-60,"elapsed":43969,"user":{"displayName":"Gladys Mawarni","userId":"03879441175200282144"}},"outputId":"e13b1e90-8539-464c-cf50-48a3e60e0fe5"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your nickname: user\n","[1] update portfolio [2] evaluate portfolio [3] Consult tax law\n","1\n","Enter the asset name: NVDA\n","Enter the number of units: 32\n","Enter the average cost per unit (USD): 101\n"]}]},{"cell_type":"code","source":["start()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7FbE7ZjX1AT","executionInfo":{"status":"ok","timestamp":1729073485693,"user_tz":-60,"elapsed":15568,"user":{"displayName":"Gladys Mawarni","userId":"03879441175200282144"}},"outputId":"a6087642-e96d-48ee-b47c-a0cec7189b8c"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your nickname: user\n","[1] update portfolio [2] evaluate portfolio [3] Consult tax law\n","1\n","Enter the asset name: GOOGL\n","Enter the number of units: 40\n","Enter the average cost per unit (USD): 99\n"]}]},{"cell_type":"code","source":["start()"],"metadata":{"id":"w6Aw39raN7Rz","colab":{"base_uri":"https://localhost:8080/","height":555},"executionInfo":{"status":"ok","timestamp":1729073529449,"user_tz":-60,"elapsed":18541,"user":{"displayName":"Gladys Mawarni","userId":"03879441175200282144"}},"outputId":"e56dee51-894a-492e-c4f5-9fa92603a01d"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your nickname: user\n","[1] update portfolio [2] evaluate portfolio [3] Consult tax law\n","2\n"]},{"output_type":"stream","name":"stderr","text":["[*********************100%***********************]  3 of 3 completed\n"]},{"output_type":"stream","name":"stdout","text":["\n","Portfolio summary for user:\n","Total Portfolio Value: $11960.24\n","Total Gain/Loss: $3700.24\n","Asset: NVDA, Units: 32, Avg Cost: 101.0, Current Price: 131.6, Current Value: 4211.2, Gain/Loss: 979.2\n","Asset: GOOGL, Units: 40, Avg Cost: 99.0, Current Price: 165.46, Current Value: 6618.400000000001, Gain/Loss: 2658.4\n","Asset: DIS, Units: 12, Avg Cost: 89.0, Current Price: 94.22, Current Value: 1130.6399999999999, Gain/Loss: 62.64\n","\n","Portfolio Analysis:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Looking at your portfolio composition and taking the modern portfolio theory into perspective, I have gathered a few recommendations for you. \n\n1. Diversify Investments: Given the high risk associated with NVDA due to its high variance as indicated by the covariance matrix, consider diversifying your investments more into stocks such as DIS and GOOGL that present lower variance and thus lower risk.\n\n2. Review Earnings Per Share: Currently, the stock with the highest EPS in your portfolio is GOOGL, which indicates its profitability. Thus, investing more in GOOGL may yield better dividends for your portfolio.\n\n3. Keep an eye on market indicators: Looking at current prices and market caps, NVDA and GOOGL seem to be the largest companies representing your portfolio. However, remember that this does not ensure they are the most profitable or the least risky. Use indicators such as EPS, P/E ratios, dividend yields, and 52-week high/low prices to better gauge each investment's potential.\n\n4. Risk-Reward tradeoff: The risk-reward tradeoff is also vital while considering investment choices. NVDA seems to provide higher expected returns but at a higher risk. On the other hand, GOOGL and DIS seem to provide moderate returns for comparatively lower risk.\n\n5. Rebalance Portfolio: Lastly, keep revisiting and rebalancing your portfolio based on market dynamics. By doing so, one can adapt to market conditions and optimize returns. \n\nPlease consult with a financial advisor before making any decision, as they can provide personalized guidance based on your financial goals and risk tolerance.\n  \nRemember that investing always involves risks and it is essential to conduct thorough due diligence. \n  \n"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"FuogmfygVkXH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GM0TQwFb_DJH"},"execution_count":null,"outputs":[]}]}